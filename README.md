<div align="center">
<h1>
<b>
Language-Guided Transformer for <br> Federated Multi-Label Classification
</b>
</h1>
</div>

<!-- <p align="center"><img src="docs/model.png" width="800"/></p> -->


> **Language-Guided Transformer for Federated Multi-Label Classification**
> 
> **I-Jieh Liu**, Ci-Siang Lin, Fu-En Yang, Yu-Chiang Frank Wang

Official implementation of our work **Language-Guided Transformer for Federated Multi-Label Classification**.

### Abstract
Federated Learning (FL) is an emerging paradigm that enables multiple users to collaboratively train a robust model in a privacy-preserving manner without sharing their private data. Most existing approaches of FL only consider traditional single-label image classification, ignoring the impact when transferring the task to multi-label image classification. Nevertheless, it is still challenging for FL to deal with user heterogeneity in their local data distribution in the real-world FL scenario, and this issue becomes even more severe in multi-label image classification. Inspired by the recent success of Transformers in centralized settings, we propose a novel FL framework for multi-label classification. Since partial label correlation may be observed by local clients during training, direct aggregation of locally updated models would not produce satisfactory performances. Thus, we propose a novel FL framework of **L**anguage-**G**uided **T**ransformer (**FedLGT**) to tackle this challenging task, which aims to exploit and transfer knowledge across different clients for learning a robust global model. Through extensive experiments on various multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is able to achieve satisfactory performance and outperforms standard FL techniques under multi-label FL scenarios.

## Update
- **(2023/12/10)** Code for FedLGT is coming soon. Stay tuned!

## Setup
### TBA

## Training and Evaluation
### TBA

## Acknowledgement
### TBA

## Contact
### TBA

